# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Computes trace ratio between BTB and Omega
#'
#' @title Computes trace ratio between BTB and Omega
#' @name calculate_r
#'
#' @param BTB A \code{matrix}. Must have dimensions \eqn{(n+4) \times (n+4)}, where
#'        \eqn{n} is the total number of unique observations.
#' @param Omega A \code{matrix}. Must have dimensions \eqn{(n+4) \times (n+4)}.
#'
#' @details
#' The trace ratio between BTB and Omega, \eqn{r}, is calculated in the following manner:
#' \enumerate{
#'   \item Calculates the trace of \code{BTB} excluding the first \eqn{2} and last \eqn{3} elements. We call this \eqn{T_1}.
#'   \item Calculates the trace of \code{Omega} excluding the first \eqn{2} and last \eqn{3} elements. We call this \eqn{T_2}.
#'   \item Calculate and return \eqn{r = T_1 / T_2}
#' }
#' \eqn{r} is used in the conversion between \code{sp} and \code{lambda}.
#'
#' @return A \code{double}. The trace ratio between BTB and Omega.
#' @export
#'
#' @examples print(1)
NULL

#' Computes the inverse matrix of \eqn{B^{T}WB + \lambda \Omega}
#'
#' @title Computes the inverse matrix of \eqn{B^{T}WB + \lambda \Omega}
#' @name calculate_Q
#'
#' @param lambda A \code{double}. The computitional smoothing parameter.
#' @param BTB A \code{matrix}. The inner matrix product \eqn{B^{T}WB}.
#'        Must have dimensions \eqn{(n+4) \times (n+4)}.
#' @param Omega A \code{matrix}. The penalty matrix.
#'        Must have dimensions \eqn{(n+4) \times (n+4)}.
#'
#' @details
#' Calculates \eqn{Q = (B^{T}WB + \lambda \Omega)^{-1}}.
#'
#' @return A \eqn{(n+4) \times (n+4)} \code{matrix}, which we denote \eqn{Q}.
#' @export
#'
#' @examples print(1)
NULL

#' Calculates the Smoother Matrix \eqn{S}
#'
#' @title Calculates the Smoother Matrix \eqn{S}
#' @name calculate_S
#'
#' @param lambda A \code{double}. The computitional smoothing parameter.
#' @param weights A \code{numeric} vector of weights. Must be of length \eqn{n}.
#' @param B A \code{matrix}. The B-spline basis matrix.
#'        Must have dimensions \eqn{(n) \times (n+4)}.
#' @param BTB A \code{matrix}. The inner matrix product \eqn{B^{T}WB}.
#'        Must have dimensions \eqn{(n+4) \times (n+4)}.
#' @param Omega A \code{matrix}. The penalty matrix.
#'        Must have dimensions \eqn{(n+4) \times (n+4)}.
#'
#' @details
#' Calculates the smoother matrix by \eqn{S = B(B^{T}WB + \lambda \Omega)^{-1}B^{T}}.
#'
#' @return A \eqn{(n) \times (n)} \code{matrix}, which we denote \eqn{S}.
#' @export
#'
#' @examples print(1)
NULL

#' Calculates the Smoother Matrix \eqn{S}
#'
#' @title Calculates the Smoother Matrix \eqn{S}
#' @name calculate_S2
#'
#' @param weights A \code{numeric} vector of weights. Must be of length \eqn{n}.
#' @param B A \code{matrix}. The B-spline basis matrix.
#'        Must have dimensions \eqn{(n) \times (n+4)}.
#' @param Q A \code{matrix}. The matrix \eqn{Q = (B^{T}WB + \lambda \Omega)^{-1}}.
#'        Must have dimensions \eqn{(n+4) \times (n+4)}.
#'
#' @details
#' Calculates the smoother matrix by \eqn{S = BQB^{T}}.
#'
#' @return A \eqn{(n) \times (n)} \code{matrix}, which we denote \eqn{S}.
#' @export
#'
#' @examples print(1)
NULL

#' Calculates the matrix part, \eqn{L}, of the covariance matrix \eqn{\hat{\sigma}^2 \cdot L}
#'
#' @title Calculates the matrix part, \eqn{L} of the covariance matrix \eqn{\hat{\sigma}^2 \cdot L}
#' @name calculate_cov_beta
#'
#' @param BTB A \code{matrix}. The inner matrix product \eqn{B^{T}WB}.
#'        Alternatively, The inner matrix product \eqn{B^{T}WWB}.
#'        Must have dimensions \eqn{(n+4) \times (n+4)}.
#'
#' @param Q A \code{matrix}. The matrix \eqn{Q = (B^{T}WB + \lambda \Omega)^{-1}}.
#'        Must have dimensions \eqn{(n+4) \times (n+4)}.
#'
#' @details
#' Calculates \eqn{L} in the covariance matrix of the smoothing spline coefficients
#'
#' \eqn{\mathrm{Cov}[\hat{\beta}] = \hat{\sigma}^2 \cdot L}
#'
#' Where \eqn{L = QB^{T}WBQ}.
#'
#' @return A \eqn{(n+4) \times (n+4)} \code{matrix}, which we denote \eqn{L}.
#' @export
#'
#' @examples print(1)
NULL

#' Calculates the variance of predicted values
#'
#' @title Calculates the variance of predicted values
#' @name calculate_pred_var
#'
#' @param B_new A \code{matrix}. The possibly new B-spline basis matrix or its derivative.
#'        Must have dimensions \eqn{(m) \times (n+4)}.
#' @param cov_beta A \code{matrix}. The matrix \eqn{\hat{\sigma}^2 \cdot L}.
#'        Must have dimensions \eqn{(n+4) \times (n+4)}. See details
#'
#' @details
#' Calculates estimated variances of smoothing spline predicted values.
#' In fact we calculate the diagonal elements of
#'
#' \eqn{\mathrm{Cov}[\hat{\beta}] = \hat{\sigma}^2 \cdot L}
#'
#' Where \eqn{L = QB^{T}W_{a}BQ}, where
#' \itemize{
#'   \item \eqn{Q = (B^{T}WB + \lambda \Omega)^{-1}}
#'   \item \eqn{W_{a}} is either \eqn{W} or \eqn{WW^{T}}
#'   \item \eqn{B} is the original \eqn{n \times (n+4)} B-spline matrix.
#'   \item \eqn{\hat{\sigma}^2 = \mathrm{WRSS} / (n - \mathrm{df})}
#' }
#'
#' @return A \code{numeric} vector of length \eqn{m}. The estimated variances of
#'         the predicted values based on new observations.
#' @export
#'
#' @examples print(1)
NULL

#' Calculates the effective degrees of freedom \eqn{\mathrm{df}}
#'
#' @title Calculates the effective degrees of freedom \eqn{\mathrm{df}}
#' @name calculate_df
#'
#' @param lambda A \code{double}. The computitional smoothing parameter.
#' @param weights A \code{numeric} vector of weights. Must be of length \eqn{n}.
#' @param B A \code{matrix}. The B-spline basis matrix.
#'        Must have dimensions \eqn{(n) \times (n+4)}.
#' @param BTB A \code{matrix}. The inner matrix product \eqn{B^{T}WB}.
#'        Must have dimensions \eqn{(n+4) \times (n+4)}.
#' @param Omega A \code{matrix}. The penalty matrix.
#'        Must have dimensions \eqn{(n+4) \times (n+4)}.
#'
#' @details
#' Calculates the effective degrees of freedom \eqn{\mathrm{df}} by \eqn{\mathrm{tr}[B(B^{T}WB + \lambda \Omega)^{-1}B^{T}]}.
#' In other words, \eqn{\mathrm{df}} is just the trace of the smoother matrix \eqn{S}.
#'
#' @return A \code{double}. The effective degrees of freedom for the smoothing spline.
#' @export
#'
#' @examples print(1)
NULL

calculate_r <- function(BTB, Omega) {
    .Call(`_smooth_commutability_calculate_r`, BTB, Omega)
}

calculate_Q <- function(lambda, BTB, Omega) {
    .Call(`_smooth_commutability_calculate_Q`, lambda, BTB, Omega)
}

calculate_S <- function(lambda, weights, B, BTB, Omega) {
    .Call(`_smooth_commutability_calculate_S`, lambda, weights, B, BTB, Omega)
}

calculate_S2 <- function(weights, B, Q) {
    .Call(`_smooth_commutability_calculate_S2`, weights, B, Q)
}

calculate_cov_beta <- function(BTB, Q) {
    .Call(`_smooth_commutability_calculate_cov_beta`, BTB, Q)
}

calculate_pred_var <- function(B_new, cov_beta) {
    .Call(`_smooth_commutability_calculate_pred_var`, B_new, cov_beta)
}

calculate_df <- function(lambda, weights, B, BTB, Omega) {
    .Call(`_smooth_commutability_calculate_df`, lambda, weights, B, BTB, Omega)
}

#' Numerical Gradient Approximation Using Richardson Extrapolation
#'
#' @title Approximate Numerical Gradient for a Function \code{f}
#' @name numerical_gradient
#'
#' @param x A \code{double}. The point where \code{f} is to be evaluated.
#' @param f A \code{function}. If the function have more than 1 argument, ensure that these are fixed.
#' @param eps A \code{double}. The delta value in which the gradient are approximated.
#' @param r An \code{integer}. The number of Iterations utilized for the Richardson extrapolation algorithm.
#'
#' @details
#' Given a particular vale of \code{x}, approximates the gradient of \code{f(x)} at this point.
#'
#' @return A \code{double} that signify the approximated gradient of \code{f} evaluated at \code{x}.
#' @export
#'
#' @examples
#' example_fun <- function(x, a = 3){return((x - a)^2)}
#' gradient_at_minimum <- numerical_gradient(3, function(x) example_fun(3, 3))
#'
#' # Expected to be approximately '0'
#' print(gradient_at_minimum)
NULL

#' Numerical Minimization Using Brent's Algorithm
#'
#' @title Obtain the value of \code{x} so that \code{f(x)} is a local minimum
#' @name brent_min
#'
#' @param f A \code{function}. Must be defined and continuous in the domain \code{[a, b]}. If the function have more than 1 argument, ensure that these are fixed.
#' @param a A \code{double}. The lower bound for the search interval.
#' @param b A \code{double}. The upper bound for the search interval.
#' @param tol A \code{double}. The desired accuracy.
#' @param max_iter An \code{integer}. The maximum number of iteration that should be performed before the algorithm forces to quit.
#'
#' @details
#' Given a particular function \code{f}, obtains a local minimum of \code{f} in the domain \code{[a, b]}. Brent's algorithm uses a combination of golden section search and quadratic interpolation to obtain the minimum.
#'
#' @return A \code{double} that signify the value of \code{x} that results in local minimum of \code{f}.
#' @export
#'
#' @examples
#' example_fun <- function(x, a = 3){return((x - a)^2)}
#' the_minimum <- brent_min(f = function(x) example_fun(x, 3), a = -3, b = 6)
#'
#' # Expected to be approximately '3'
#' print(the_minimum)
#'
#' # The value of f at the minimum
#' print(example_fun(the_minimum))
NULL

#' Numerical Initial Minimization Using a Stepwise Approach
#'
#' @title Obtain the Minimum Value of \code{x} Where \code{f(x)} Reaches a Minimum
#' @name first_min
#'
#' @param f A \code{function}. Must be defined and continuous in the domain \code{[a, b]}. If the function have more than 1 argument, ensure that these are fixed.
#' @param a A \code{double}. The lower bound for the search interval.
#' @param b A \code{double}. The upper bound for the search interval.
#' @param tol A \code{double}. The desired accuracy.
#' @param step_init A \code{double}. The initial step size multiplier. This number of multiplied by the length of \code{[a, b]} to get the initial step size \eqn{\Delta_0}.
#' @param max_iter An \code{integer}. The maximum number of iteration that should be performed before the algorithm forces to quit.
#'
#' @details
#' Given a particular function \code{f}, obtains the first local minimum of \code{f} in the domain \code{[a, b]}.
#' For each iteration, we check whether \eqn{f'(\cdot) > 0}.
#' The i-th search interval is given by \eqn{[a + \Delta_0 \sum_{j=0}^{i-2} 2^{j}, a + \Delta_0 \sum_{j=0}^{i-1} 2^{j}\,]}.
#' If a minimum is not found before \eqn{a + \Delta_0 \sum_{j=0}^{i-1} 2^{j} > b}, \code{brent_min} is called instead.
#' Note that this algorithm works best if \code{f} is a differentiable and smooth function. Note also that If \eqn{f'(a) > 0}, this algorithm will
#' return \code{a}, so this algorithm is most useful if \eqn{f'(x) < 0} in the start of the interval \code{[a, b]}.
#'
#' @return A \code{double} that signify the value of \code{x} that results in the initial minimum of \code{f}.
#' @export
#'
#' @examples
#' example_fun <- function(x, a = 3){return((x - a)^2)}
#' the_first_minimum <- first_min(f = function(x) example_fun(x, 3), a = -3, b = 6)
#'
#' # Expected to be approximately '3'
#' print(the_first_minimum)
#'
#' # The value of f at the minimum
#' print(example_fun(the_first_minimum))
NULL

numerical_gradient <- function(x, f, eps = 1e-4, r = 4L) {
    .Call(`_smooth_commutability_numerical_gradient`, x, f, eps, r)
}

brent_min <- function(f, a, b, tol = 1.22e-4, max_iter = 100L) {
    .Call(`_smooth_commutability_brent_min`, f, a, b, tol, max_iter)
}

first_min <- function(f, a, b, tol = 1e-8, step_init = 1e-4, max_iter = 1000L) {
    .Call(`_smooth_commutability_first_min`, f, a, b, tol, step_init, max_iter)
}

#' Computes the absolute effective degrees of freedom error
#'
#' @title Computes the absolute effective degrees of freedom error
#' @name error_df2
#'
#' @param sp A \code{double}, and is a scale-free variant of the computational lambda. Given this value of \code{lambda}, the trace of the smoother matrix is calculated and the absolute deviation between this and the given \code{df} is recorded.
#' @param df A \code{double} that corresponds to the target effective degrees of freedom for the smoothing spline model.
#' @param weights A \code{numeric} vector sorted according to the order of the predictor \code{x}.
#' @param B A B-spline basis \code{matrix} evaluated at the sorted and unique values of the predictor values, \code{x}.
#' @param BTB A \code{matrix} that is the matrix product between the transpose of \code{B} and \code{B}.
#' @param Omega The O'Sullivan penalty \code{matrix} for the B-spline basis.
#' @param r The trace ratio of the matrices
#'
#' @description
#' Given a value of \code{lambda}, the corresponding effective degrees of freedom is calculated. This value of the degrees of freedom is compared to \code{df}. This comparison is done by taking the absolute difference between the effective degrees of freedom associated with the \code{lambda} given and the provided value of \code{df}. This function is primarily used to convert a value of \code{df} to a suitable value of \code{lambda} using numerical optimization. See \code{to_lambda} for more details.
#'
#'
#' @return A \code{double} that signify the absolute deviation between the trace of the smoother matrix (given the value of \code{lambda}) and the provided value of \code{df}.
#' @export
#'
#' @examples print(1)
NULL

error_df2 <- function(sp, df, weights, B, BTB, Omega, r = 1.0) {
    .Call(`_smooth_commutability_error_df2`, sp, df, weights, B, BTB, Omega, r)
}

#' Calculate a Statistic for Each Sample
#'
#' @title Calculate a Statistic for Each Sample
#' @name fun_of_replicates2
#'
#' @param data A \code{data.table} or \code{list} object. Must contain
#'        \code{SampleID}, \code{ReplicateID}, \code{MP_A} and \code{MP_B}.
#' @param fun A \code{character} string. Which statistic is to be calculated for
#'        each SampleID. Possible choices include \code{mean}, \code{var} (variance),
#'        \code{sd} (standard deviaton), \code{cv} (coefficient of variation),
#'        \code{median}, \code{min} (minimum) and \code{max} (maximum).
#'
#' @description Calculates a chosen statistic over the replicates.
#'
#' @details This function handles NA-values automatically.
#'
#' @return A \code{list} with elements \code{SampleID}, \code{MP_A} and \code{MP_B}.
#'         \code{MP_A} and \code{MP_B} contains the sample-wise statistics.
#'
#' @examples \dontrun{
#'   print(1)
#' }
fun_of_replicates2 <- function(data, fun = "mean") {
    .Call(`_smooth_commutability_fun_of_replicates2`, data, fun)
}

#' Checks if \eqn{y_{n+j}} for \eqn{j = 1, \ldots, m} are inside Deming prediction intervals.
#'
#' @title Check Whether New Observations Are Inside Deming Prediction Intervals
#' @name inside_deming2
#'
#' @param data A \code{list} or \code{data.table}. Must contain \code{MP_A} (response variable values)
#'        and \code{MP_B} (predictor variable values).
#' @param new_data A \code{list} or \code{data.table}. Must contain \code{MP_A} (new response variable values)
#'        and \code{MP_B} (new predictor variable values).
#' @param imprecision_estimates A \code{list} or \code{data.table}. Must contain \code{lambda}
#'        and \code{Var_B}. See details for more information.
#' @param R A \code{integer} value. The number of replicated measurements. If the number of
#'        replicated measurements is sample-dependent, use some summary statistic.
#'        Defaults to \code{3}.
#' @param R_ratio A \code{double} value. If a different number of replicated measurements are used
#'        in data than in new_data, specify the ratio here. Defaults to \code{1}.
#' @param level A \code{double} value. The desired nominal confidence level for
#'        the estimated Deming prediction intervals. Must be between \code{0} and \code{1}.
#'        Sensible values include values larger than or equal to \code{0.50}. Defaults to \code{0.99}.
#'
#'
#' @details
#' The Deming regression \eqn{\mathrm{level} \cdot 100\%} prediction intervals are calculated by
#'
#' \eqn{PI_D(y_{n+j} | x_{n+j}) \approx b_0 + b_1 \cdot x_{n+j} \pm t_{n - 2, 1 - \alpha/2} \sqrt{\hat{\sigma}_{b_1}^2\Big[(\hat{x}_{n+j}^L - \hat{\mu})^2 + \tilde{\sigma}_h^2 \cdot R_{\mathrm{rat}}\Big] + R_{\mathrm{rat}} \cdot \Big(\frac{n+j}{n}\Big) \cdot (\tilde{\sigma}_v^2 + b_1^2 \cdot \tilde{\sigma}_h^2)}}
#'
#' This function calculates \eqn{z_j = \mathbb{I}\big[y_{n+j} \in PI_D(y_{n+j} | x_{n+j})\big]}
#' for each \eqn{j = 1, \ldots, m}. Note that
#' \itemize{
#'   \item \eqn{b_1} is the Deming regression slope estimate of \eqn{\beta_1}.
#'   \item \eqn{\hat{\sigma}_{b_1}^2} is the estimated variance of the Deming regression slope estimator \eqn{b_1}.
#'   \item \eqn{\hat{x}_{n+j}^L} is an estimate of the latent \eqn{x_{n+j}^L}.
#'   \item \eqn{\hat{\mu}} is an estimate of the mean of \eqn{x_{i}^L}.
#'   \item \eqn{\tilde{\sigma}_h^2} is the method of moments estimator of the repeatability variance \eqn{\sigma_h^2}.
#'   \item \eqn{\tilde{\sigma}_v^2} is the method of moments estimator of the repeatability variance \eqn{\sigma_v^2}.
#'   \item \eqn{R_{\mathrm{rat}}} is the \code{R_ratio} value.
#' }
#'
#' Missing values in \code{MP_A} or \code{MP_B} in \code{data} are allowed and will be silently removed
#' before \eqn{z_j} values are calculated. Missing values in \code{MP_A} or \code{MP_B} in \code{new_data}
#' are allowed, but corresponding \eqn{z_j} values will not be calculated in such cases.
#'
#' Note that \code{imprecision_estimates} should contain the following variables:
#' \itemize{
#'   \item \code{lambda}: This an estimator of the ratio \eqn{\sigma_v^2 / \sigma_h^2}.
#'         How you estimate this ratio is up to you. However, a natural estimator is
#'         \eqn{\hat{\sigma}_v^2 / \hat{\sigma}_h^2}, where \eqn{\hat{\sigma}_v^2} and
#'         \eqn{\hat{\sigma}_h^2} are pooled variances based on replicated measurements.
#'   \item \code{Var_B}: This is an estimator of \eqn{\sigma_h^2}. How you estimate this
#'         ratio is up to you. However a natural estimator is \eqn{\hat{\sigma}_h^2},
#'         which is a pooled variance based on replicated measurements.
#' }
#' N.B., if either \code{lambda} or \code{Var_B} is missing or takes negative values
#' Ordinary least squares regression is used instead to calculate \eqn{z_j} values.
#'
#'
#' @return An \code{integer} vector of length equal to the number of rows in \code{new_data}.
#'
#' @examples \dontrun{
#'   print(1)
#' }
inside_deming2 <- function(data, new_data, imprecision_estimates, R = 3L, R_ratio = 1.0, level = 0.99) {
    .Call(`_smooth_commutability_inside_deming2`, data, new_data, imprecision_estimates, R, R_ratio, level)
}

#' Calculates the Deming regression estimate of \eqn{\sigma_h^2}.
#'
#' @title Calculates the Deming Regression Estimate of \eqn{\sigma_h^2}.
#' @name sigma_h_squared_deming2
#'
#' @param data A \code{list} or \code{data.table}. Must contain \code{MP_A} (response variable values)
#'        and \code{MP_B} (predictor variable values).
#' @param lambda A \code{double} value.
#'
#' @details
#' This function is typically not relevant for end-users.
#'
#'
#' @return An \code{double} that is the Deming regression estimate of \eqn{\sigma_h^2}.
#'
#' @examples \dontrun{
#'   print(1)
#' }
sigma_h_squared_deming2 <- function(data, lambda = 1) {
    .Call(`_smooth_commutability_sigma_h_squared_deming2`, data, lambda)
}

#' Estimate weighted local average curve.
#'
#' @title Estimate Weighted Local-Average Curve
#' @name local_average
#'
#' @param x A \code{numeric} vector. Must contain the stochastic process values
#'        that are used in estimating the weighted local average curve.
#' @param weights A \code{numeric} vector. Must contain weights for each of the stochastic
#'        process values. Set all weights to \code{1} to bypass weighting.
#' @param window A \code{integer} value. The window-width for estimating averages
#'        at each point. See details.
#'
#' @details
#' Calculates the estimated weighted local-averages for the sample \eqn{\lbrace x_i \rbrace_{i=1}^{N}}
#' and weights \eqn{\lbrace w_i \rbrace_{i=1}^{N}}.
#' We assume the following model:
#'
#' \eqn{x_i = f(x_i) + \epsilon_i}.
#'
#' We attempt to estimate the signal \eqn{f(x_i)}, using a weighted local-average model:
#'
#' \eqn{\hat{f}(x_i) = \frac{1}{w_i \cdot (\min\lbrace N, i + \Delta \rbrace - \max\lbrace 1, i - \Delta \rbrace)} \cdot \sum_{j = \max\lbrace 1, i - \Delta \rbrace}^{\min\lbrace N, i + \Delta \rbrace} x_j}
#'
#' Here, \eqn{\Delta} is half of the value given in \code{window}. \code{NA} values are allowed,
#' and will be silently removed if present.
#'
#' @return A \code{numeric} vector of local-average estimates.
#'
#' @examples \dontrun{
#'   print(1)
#' }
local_average <- function(x, weights, window) {
    .Call(`_smooth_commutability_local_average`, x, weights, window)
}

#' N-fold cross-validation estimate of the mean squared prediction error of the smoothing spline model
#'
#' @title N-fold cross-validation estimate of the mean squared prediction error of the smoothing spline model
#' @name nfold_cv2
#'
#' @param sp A non-negative \code{double}. This is a scaled version of the raw smoothing parameter lambda and is usually what is optimized.
#' @param y A \code{numeric} vector sorted according to the order of the predictor \code{x}.
#' @param weights A \code{numeric} vector sorted according to the order of the predictor \code{x}.
#' @param B A B-spline basis \code{matrix} evaluated at the sorted and unique values of \code{x} .
#' @param BTB A \code{matrix} that is the matrix product between the transpose of \code{B} and \code{B}.
#' @param Omega The penalty \code{matrix} for the B-spline basis.
#' @param cross_validation A \code{logical} value. If set to \code{TRUE}, you are effectively prompting that the lambda inputs are equivalent to the spar smoothing parameter.
#' @param r The trace ratio of the matrices
#'
#' @return A \code{double} that signify the n-fold cross-validation score
#' @export
#'
#' @examples print(1)
NULL

#' Generalized cross-validation estimate of the mean squared prediction error of the smoothing spline model
#'
#' @title Generalized cross-validation estimate of the expected mean squared prediction error of the smoothing spline model
#' @name gcv2
#'
#' @param sp A non-negative \code{double}. This is a scaled version of the raw smoothing parameter lambda and is usually what is optimized.
#' @param y A \code{numeric} vector sorted according to the order of the predictor \code{x}.
#' @param weights A \code{numeric} vector sorted according to the order of the predictor \code{x}.
#' @param B A B-spline basis \code{matrix} evaluated at the sorted and unique values of \code{x} .
#' @param BTB A \code{matrix} that is the matrix product between the transpose of \code{B} and \code{B}.
#' @param Omega The penalty \code{matrix} for the B-spline basis.
#' @param cross_validation A \code{logical} value. If set to \code{TRUE}, you are effectively prompting that the lambda inputs are equivalent to the spar smoothing parameter.
#' @param r The trace ratio of the matrices
#' @param smudge A \code{double} larger than or equal to 1. Set to \code{1.2} or
#'        \code{1.4} to avoid undersmoothing.
#'
#' @return A \code{double} that signify the GCV cross-validation score.
#' @export
#'
#' @examples print(1)
NULL

nfold_cv2 <- function(sp, y, weights, B, BTB, Omega, cross_validation = FALSE, r = 1.0) {
    .Call(`_smooth_commutability_nfold_cv2`, sp, y, weights, B, BTB, Omega, cross_validation, r)
}

gcv2 <- function(sp, y, weights, B, BTB, Omega, cross_validation = FALSE, r = 1.0, smudge = 1.4) {
    .Call(`_smooth_commutability_gcv2`, sp, y, weights, B, BTB, Omega, cross_validation, r, smudge)
}

#'
#' @title Obtain optimal upper limit for constrained search grid for optimal df
#' @name obtain_df_max
#'
#' @param df A \code{numeric} vector signifying the grid in which second derivatives of zeta are evaluated
#' @param m A \code{numeric} vector signifying the evaluated second derivatives of zeta
#' @param threshold A \code{double}. Which relative value of the maximum found second derivative should be used as tolerance. Must be between 0 and 1.
#'
#' @description Computes the first value of \code{df} where the curve of zeta starts to increase after initial decrease.
#'
#' @details Used to constrain the search grid for an optimal value of \code{df}.
#'
#' @return A \code{double}. The upper limit for the constrained search grid for optimal \code{df}.
#'
#' @examples \dontrun{
#'   print(1)
#' }
NULL

obtain_df_max <- function(df, second_deriv, threshold = 0.05) {
    .Call(`_smooth_commutability_obtain_df_max`, df, second_deriv, threshold)
}

#' Reconstruct a 4-banded matrix based on flattened data
#'
#' @title Reconstruct a 4-banded matrix based on flattened data
#' @name reconstruct_4_band_matrix
#'
#' @param x A \code{numeric} vector signifying the flattened version of the 4-banded matrix.
#'
#' @description Reconstructs a 4-banded matrix based on flattened data.
#'
#' @details This reconstruction assumes symmetrical 4-banded matrix. End-users should not bother with this function.
#'
#' @return A 4-banded \code{matrix}.
#'
#' @examples \dontrun{
#'   print(1)
#' }
reconstruct_4_band_matrix <- function(x) {
    .Call(`_smooth_commutability_reconstruct_4_band_matrix`, x)
}

#' Reconstruct matrices based on smooth.spline output
#'
#' @title Reconstruct matrices based on smooth.spline output
#' @name get_matrices
#'
#' @param auxM A \code{List} containing the \code{auxM} object from the smooth.spline fit.
#'
#' @description Reconstructs the penalty and B-spline matrices based on flattened data.
#'
#' @details This reconstruction assumes symmetrical 4-banded matrices. End-users should not bother with this function.
#'
#' @return A list of two 4-banded \code{matrix} objects and one \code{numeric} vector.
#'
#' @examples \dontrun{
#'   print(1)
#' }
get_matrices <- function(auxM) {
    .Call(`_smooth_commutability_get_matrices`, auxM)
}

#' Calculate imprecision point estimates of measurements in a given IVD-MD comparison
#'
#' @title Calculate imprecision point estimates of measurements in a given IVD-MD comparison
#' @name global_precision_estimates2
#'
#' @param data \code{list} or \code{data.table}. Must contain the following variables:
#' \itemize{
#'   \item \code{SampleID:} Sample identifiers. Must be a \code{character} vector.
#'   \item \code{ReplicateID:} Replicate measurement identifiers within samples. Must be \code{character} vector.
#'   \item \code{MP_A:} Measurement results for the IVD-MD used as response variable.
#'   \item \code{MP_B:} Measurement results for the IVD-MD used as predictor variable.
#' }
#'
#' @details
#' Calculates five relevant global imprecision estimates. The term global is
#' used because these imprecision estimates are based on the whole dataset.
#' These five imprecision estimates are calculated:
#' \itemize{
#'   \item \code{Var_A:} Pooled variance of all sample-variances based on \code{MP_A}
#'   \item \code{Var_B:} Pooled variance of all sample-variances based on \code{MP_B}
#'   \item \code{CV_A:} CV estimate based on Var_A and the grand mean of all measurements from \code{MP_A}
#'   \item \code{CV_B:} CV estimate based on Var_B and the grand mean of all measurements from \code{MP_B}
#'   \item \code{lambda:} Ratio of pooled variances \code{Var_A} and \code{Var_B}
#' }
#' Coefficient of Variation (CV) values \code{CV_A} and \code{CV_B} can also be represented as percentages.
#' To convert to percentage values, multiply their raw results by 100.
#'
#' @return A \code{list} of length \code{5} with the following point imprecision estimates:
#'         \code{Var_A}, \code{Var_B}, \code{CV_A}, \code{CV_B} and \code{lambda}.
#'         See details for more information on these statistics.
#'
#' @examples \dontrun{
#'   library(data.table)
#'   data <- simulate_eqa_data2(list(n = 25, R = 3, cvx = 0.02, cvy = 0.3), AR = TRUE)
#'   data$SampleID <- as.character(data$SampleID)
#'   data$ReplicateID <- as.character(data$ReplicateID)
#'   print(global_prcision_estimates2(data = data) |> as.data.table())
#' }
NULL

#' Resample clustered EQA clinical sample data
#'
#' @title Resample clustered EQA clinical sample data
#' @name resample_samples2
#'
#' @param data A \code{list} or a \code{data.table}. Must contain \code{SampleID},
#'        \code{ReplicateID}, \code{MP_A} and \code{MP_B}. The ID variables \code{SampleID}
#'        and \code{ReplicateID} must be of character type for the function to operate correctly.
#'
#' @details
#' This function is a very efficient method to resample clinical sample data on sample-level.
#' It is convenient to combine this function with \code{fasteqa} functions such as
#' \itemize{
#'   \item \code{global_precision_estimates()} to estimate bootstrap imprecision confidence intervals
#'   \item \code{estimate_zeta()} to estimate bootstrap zeta confidence intervals
#' }
#' Alternatively it could be used to estimate \code{smooth.commutability} functions such as
#' \itemize{
#'   \item \code{estimate_zeta_ss()} to estimate bootstrap smoothing spline zeta confidence intervals
#'   \item \code{estimate_df()} to estimate bootstrap smoothing spline df confidence intervals
#' }
#'
#' @return A \code{list} containing the resampled EQA clinical sample data.
#'
#' @examples
#' library(data.table)
#' fictive_data <- simulate_eqa_data2(list(n = 25, R = 3, cvx = 0.01, cvy = 0.01), AR = TRUE)
#' resampled_data <- resample_samples2(fictive_data)
#' setDT(resampled_data)
#' print(resampled_data)
NULL

#' Resample cluster statistics based on EQA clinical sample data
#'
#' @title Resample cluster statistics based on EQA clinical sample data
#' @name resample_fun_of_samples
#'
#' @param data A \code{list} or a \code{data.table}. The mean-of-replicates
#'        clinical sample data. Must contain \code{SampleID}, \code{MP_A} and \code{MP_B}.
#'        The ID variable \code{SampleID} must be \code{character}.
#'
#' @details
#' This function is a very efficient method to resample aggregated sample statistics
#' based on clinical sample data.
#'
#' It is convenient to combine this function with \code{smooth.commutability} functions such as
#' \itemize{
#'   \item \code{predict_smoothing_splines()} to estimate inside rates for a IVD-MD comparison
#'   \item \code{smoothing_spline()} to estimate bootstrap distribution of LOO-CV chosen effective degrees of freedom.
#' }
#'
#' @return
#' A \code{list} that contains the resampled \code{data}.
#'
#' @examples
#' library(data.table)
#' fictive_data <- simulate_eqa_data2(list(n = 25, R = 3, cvx = 0.01, cvy = 0.02))
#' resampled_data <- resample_fun_of_samples(fictive_data)
#' setDT(resampled_data)
#' print(resampled_data)
NULL

#' Resample cluster statistics based on EQA clinical sample data for each IVD-MD comparison
#'
#' @title Resample cluster statistics based on EQA clinical sample data for each IVD-MD comparison
#' @name resample_fun_of_samples_all
#'
#' @param data A \code{list} or a \code{data.table}. The mean-of-replicates (MOR)
#'        clinical sample data. Must contain \code{comparison} \code{SampleID},
#'        \code{MP_A} and \code{MP_B}. The ID variables \code{comparison}
#'        and \code{SampleID} must be \code{character}.
#'
#' @details
#' This function is a very efficient method to resample aggregated sample statistics
#' based on clinical sample data grouped by IVD-MD \code{comparison}.
#'
#' It is convenient to combine this function with \code{smooth.commutability} functions such as
#' \itemize{
#'   \item \code{predict_smoothing_splines()} to estimate inside rates for each IVD-MD comparison
#'   \item \code{smoothing_spline()} to estimate bootstrap distribution of LOO-CV chosen effective degrees of freedom.
#' }
#'
#' @return
#' A \code{list} that contains the resampled \code{data}.
#'
#' @examples
#' library(data.table)
#' fictive_data1 <- simulate_eqa_data2(list(n = 25, R = 3, cvx = 0.01, cvy = 0.02))
#' fictive_data2 <- simulate_eqa_data2(list(n = 25, R = 3, cvx = 0.03, cvy = 0.02))
#' fictive_data1$comparison <- rep("A - B", length(fictive_data1$MP_B))
#' fictive_data2$comparison <- rep("A - C", length(fictive_data2$MP_B))
#' fictive_data <- rbindlist(list(fictive_data1, fictive_data2))
#' resampled_data <- resample_fun_of_samples_all(fictive_data)
#' setDT(resampled_data)
#' print(resampled_data)
NULL

#' Resample imprecision estimates based on clustered EQA clinical sample data
#'
#' @title Resample imprecision estimates based on clustered EQA clinical sample data
#' @name resample_imprecision
#'
#' @param data A \code{list} or a \code{data.table}. Must contain \code{SampleID},
#'        \code{ReplicateID}, \code{MP_A} and \code{MP_B}. The ID variables \code{SampleID}
#'        and \code{ReplicateID} must be of character type for the function to operate correctly.
#'
#' @details
#' This function is a very efficient method to resample repeatability measure estimates
#' based on clinical sample data in \code{data}.
#'
#' @return A \code{list} containing the resampled imprecision.
#'
#' @examples
#' library(data.table)
#' fictive_data <- simulate_eqa_data2(list(n = 25, R = 3, cvx = 0.01, cvy = 0.01), AR = TRUE)
#' impr <- replicate(n = 5, expr = resample_imprecision(fictive_data), simplify = FALSE)
#' impr <- rbindlist(impr)
#' print(impr)
NULL

global_precision_estimates2 <- function(data) {
    .Call(`_smooth_commutability_global_precision_estimates2`, data)
}

resample_samples2 <- function(data) {
    .Call(`_smooth_commutability_resample_samples2`, data)
}

resample_fun_of_samples <- function(data) {
    .Call(`_smooth_commutability_resample_fun_of_samples`, data)
}

resample_fun_of_samples_all <- function(data) {
    .Call(`_smooth_commutability_resample_fun_of_samples_all`, data)
}

resample_imprecision <- function(data) {
    .Call(`_smooth_commutability_resample_imprecision`, data)
}

#' Simulation of EQA data based on study design and potential differences in non-selectivity
#'
#' @title Simulation of EQA data based on study design and potential differences in non-selectivity
#' @name simulate_eqa_data2
#'
#' @param parameters A \code{list} of parametedrs used to simulate the EQA data. You must at least specify one parameter for this function to run. Except that one mandatory parameter, you may optionally choose the remaining of the parameters. These are the optimal parameters that you may include into the list:
#' \itemize{
#'   \item \code{n:} The number of samples.
#'   \item \code{R:} The number of replicates on each sample.
#'   \item \code{cvx:} The repeatability coefficient of variation for IVD-MD \code{MP_B}.
#'   \item \code{cvy:} The repeatability coefficient of variation for IVD-MD \code{MP_A}.
#'   \item \code{cil:} The lower bound of the concentration interval
#'   \item \code{ciu:} The upper bound of the concentration interval
#'   \item \code{dist:} The distribution to simulate latent variables from.
#'                      Possbile choices include \code{unif} (uniform distribution, default),
#'                      \code{norm} (normal distribution), \code{lst} (location-scale t-distribution),
#'                      \code{lnorm} (log-normal distribution)
#'   \item \code{df_tau:} The degrees of freedom for the 'lst' distribution if the distribution of latent variables are location-scale t-distributed ('lst'). Defaults to 5 if not otherwise is specified.
#'   \item \code{eta:} The heteroscedasticity factor.
#'   \item \code{eta0:} The proportion of base MS standard deviations.
#'   \item \code{qpos:} Position of systematic differences in non-selectivity. 0 signify lower range and 1 signify upper range
#'   \item \code{qran:} Interquantile range where systematic differences in non-selectivity should have its effect
#'   \item \code{prop:} average proportion of clinical samples affected by random differences in non-selectivity
#'   \item \code{mmax:} The maximum relocation magnitude in number of analytical SDs of y measurements. This assumes either prop or qpos and qran to be specified as well
#'   \item \code{b0:} For systematic linear DINS between IVD-MDs. Intercept. Defaults to 0.
#'   \item \code{b1:} For systematic linear DINS between IVD-MDs. Slope. Defaults to 1.
#'   \item \code{c0:} For systematic linear non-selectivity in IVD-MD 1. Intercept. Defaults to 0.
#'   \item \code{c1:} For systematic linear non-selectivity in IVD-MD 1. Slope. Defaults to 1.
#'   \item \code{error_dist:} The distribution to simulate measurement error components from. Possible choices include 'norm' (normal distribution, default) and 'lt' (location t-distribution)
#'   \item \code{dfx:} The degrees of freedom for the measurement error components in IVD-MD 1 if error_dist = 'lt'. Defaults to 5 if not otherwise is specified.
#'   \item \code{dfy:} The degrees of freedom for the measurement error components in IVD-MD 2 if error_dist = 'lt'. Defaults to 5 if not otherwise is specified.
#'   \item \code{md_method:} Method for simulation missing data. Possible choices include 'none' (no missing data is simulated, default), 'mar' (missing at random), 'mnar' (missing not at random) and 'marmnar' (missing both at random and not at random)
#'   \item \code{mar_prob:} The probability (value between 0 and 1) of having a missing measurement. Only relevant if \code{md_method} is 'mar' or 'marmnar'. If not specified, but \code{md_method} = 'mar' or \code{md_method} = 'marmnar', it defaults to 0.05.
#'   \item \code{mnar_threshold:} The lower bound threshold (a real value) for when a measurement should be missing. Only relevant if \code{md_method} is 'mnar' or 'marmnar'. If not specified, but \code{md_method} = 'mnar' or \code{md_method} = 'marmnar', it defaults to \code{cil}. Alternatively, if not specified, but \code{md_method} = 'mnar0' or \code{md_method} = 'marmnar0', it defaults to 0.
#' }
#' @param type \code{integer}. Set to \code{0} for default simulation of data. Set to \code{1}, \code{2} or \code{3} to simulate from custom built in non-linear functions.
#' @param AR \code{logical}. If \code{TRUE}, data is simulated including replicated measurements. Otherwise, mean of replicated measurements are returned (MOR).
#' @param include_parameters \code{logical}. If \code{TRUE}, the used parameters in the data simulation is saved and placed in a seperate list as part of the output.
#' @param shift \code{logical}. If \code{TRUE}, the simulated data change roles. MP_A becomes MP_B, and MP_B becomes MP_A.
#'
#' @description Simulates a data set with n x R rows, and four columns. The two first columns are the base ID columns (\code{SampleID} and \code{ReplicateID}). The remaining columns are numeric columns holding measurement results from the two IVD-MDs in comparison (denoted 'MP_A' (y) and 'MP_B' (x)).
#'              The form of the simulated data depends greatly on which parameters are specified in the the \code{parameters} argument.
#' @details Simulates method comparison data for \code{n} samples (e.g., clinical samples, pools, external quality assessment samples, reference material samples), where each sample is measured \code{R} times (replicated measurements). In theory, we simulate from (x_ir, y_ir) where x_ir = f(tau_i) + h_ir and y_ir = g(f(tau_i)) + v_ir.
#'          The form of f is specified through parameters \code{c0} and \code{c1}, whereas g is specified through numerous parameters such as \code{b0}, \code{b1}, \code{qpos}, \code{qran}, \code{mmax}, \code{prop}. tau_i is modelled through \code{cil}, \code{ciu}, \code{dist} and \code{df_tau}.
#'          h_ir and v_ir are measurement error components modelled through \code{cvx}, \code{cvy}. \code{cvx}, \code{cvy} can be functions of \code{error_dist}, \code{dfx}, \code{dfy}, \code{eta} and \code{eta0}.
#'          In order to convert the outputted list to a table, use either \code{as.data.frame()}, \code{as.data.table()}, \code{as.tibble()}. The most efficient way to convert is \code{setDT()} from the \code{data.table} package.
#'
#' @return A list where each list element is a column of the generated EQA data
#'
#' @examples \dontrun{
#'
#'   # Load data.table package from library
#'   library(data.table)
#'
#'   # Simulate 25 clinical samples measured in triplicate affected by
#'   # random differences in non-selectivity
#'   parameters_css <- list(n = 25, R = 3, prop = 0.1, mmax = 5, cil = 25, ciu = 75)
#'   simulated_css <- simulate_eqa_data(parameters = parameters_css)
#'
#'   # Simulate 3 external quality assessment material samples
#'   # measured in duplicate not affected by differences in non-selectivity
#'   parameters_eqams <- list(n = 3, R = 2, b0 = 0.1, b1 = 1.1)
#'   simulated_eqams <- simulate_eqa_data(parameters = parameters_eqams)
#'
#'   # We can assume that tau_i ~ lst(df_tau = 10, mu_tau = 50, var_tau = 78.583)
#'   parameters_css <- c(parameters_css, dist = "lst", df_tau = 10)
#'   simulated_css_lst <- simulate_eqa_data(parameters = parameters_css)
#'
#'   # We can convert the list objects to data.table objects using setDT()
#'   setDT(simulated_css)
#'   setDT(simulated_eqams)
#'   setDT(simulated_css_lst)
#'
#'   # Print results
#'   print(simulated_css)
#'   print(simulated_eqams)
#'   print(simulated_css_lst)
#'
#' }
#'
NULL

simulate_eqa_data2 <- function(parameters, type = 1L, AR = FALSE, include_parameters = FALSE, shift = FALSE) {
    .Call(`_smooth_commutability_simulate_eqa_data2`, parameters, type, AR, include_parameters, shift)
}

#' Calculate sample skewness based on a numeric vector \code{x}.
#'
#' @title Calculate Sample Skewness of a Random Sample
#' @name skewness
#'
#' @param x A \code{numeric} vector that is a random sample.
#' @param na_rm A \code{logical} value. If \code{TRUE}, \code{NA}-values are
#'        removed prior to calculation of sample skewness.
#'
#' @details
#' Calculates the sample skewness of a random sample \eqn{\lbrace x_i \rbrace_{i=1}^{N}}.
#' The sample skewness, attempts to estimate the theoretical skewness, \eqn{\gamma}, by using
#' the following estimator
#'
#' \eqn{\hat{\gamma} = \frac{\sqrt{N(N-1)}}{N-2} \cdot \frac{\frac{1}{N}\sum_{i=1}^{N}(x_i - \overline{x})^3}{\Big[\frac{1}{N}\sum_{i=1}^{N}(x_i - \overline{x})^2\Big]^{1.5}}}.
#'
#' Note that this estimator will be biased if not \eqn{x_i \sim \mathrm{N}(\mu, \sigma^2)}.
#'
#' @return A \code{double} that is the calculated sample skewness.
#'
#' @examples \dontrun{
#'   y <- rlnorm(n = 1000, meanlog = 0, sdlog = 0.25)
#'   skew_y <- skewness(y)
#'   print(skew_y)
#' }
skewness <- function(x, na_rm = TRUE) {
    .Call(`_smooth_commutability_skewness`, x, na_rm)
}

#' Calculate sample excess kurtosis based on a numeric vector \code{x}.
#'
#' @title Calculate Sample Excess Kurtosis of a Random Sample
#' @name kurtosis
#'
#' @param x A \code{numeric} vector that is a random sample.
#' @param na_rm A \code{logical} value. If \code{TRUE}, \code{NA}-values are
#'        removed prior to calculation of sample excess kurtosis.
#'
#' @details
#' Calculates the sample excess kurtosis of a random sample \eqn{\lbrace x_i \rbrace_{i=1}^{N}}.
#' The sample excess kurtosis, attempts to estimate the theoretical excess kurtosis, \eqn{\Kappa}, by using
#' the following estimator
#'
#' \eqn{\hat{\Kappa} = \frac{N(N+1)}{(N-1)(N-2)(N-3)} \cdot \frac{\sum_{i=1}^{N}(x_i - \overline{x})^4}{s^4} - 3 \cdot \frac{(N-1)^2}{(N-2)(N-3)}}.
#'
#' Here, \eqn{s^2} is the unbiased sample variance.
#' Note that this estimator will be biased if not \eqn{x_i \sim \mathrm{N}(\mu, \sigma^2)}.
#' Note also that \eqn{\hat{\Kappa}} estimates the theoretical excess kurtosis and not the raw kurtosis.
#' Therefore, one should add \eqn{3} to the output to get the estimated raw kurtosis.
#'
#' @return A \code{double} that is the calculated sample excess kurtosis.
#'
#' @examples \dontrun{
#'   y <- rnorm(n = 1000)
#'   kurt_y <- kurtosis(y)
#'   print(kurt_y)
#' }
kurtosis <- function(x, na_rm = TRUE) {
    .Call(`_smooth_commutability_kurtosis`, x, na_rm)
}

